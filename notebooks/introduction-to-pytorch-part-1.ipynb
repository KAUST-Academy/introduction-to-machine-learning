{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import pickle\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble, model_selection, preprocessing\n",
    "import torch\n",
    "import torchinfo\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "* Tutorial materials are derived from [_What is torch.nn really?_](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset\n",
    "\n",
    "The original [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"../data/\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"cifar-10\"\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(RAW_DATA_DIR / \"cifar-10-python.tar.gz\", \"wb\") as f:\n",
    "    response = requests.get(URL)\n",
    "    f.write(response.content)\n",
    "\n",
    "with tarfile.open(RAW_DATA_DIR / \"cifar-10-python.tar.gz\", \"r:gz\") as f:\n",
    "    f.extractall(RAW_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "filepaths = glob.glob(\"../data/cifar-10/cifar-10-batches-py/*_batch*\")\n",
    "for filepath in sorted(filepaths):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        batch = pickle.load(f, encoding=\"latin1\")\n",
    "        data.append(batch[\"data\"])\n",
    "        labels.extend(batch[\"labels\"])\n",
    "\n",
    "# each image has 3 channels with height and width of 32 pixels\n",
    "features = pd.DataFrame(\n",
    "    np.vstack(data),\n",
    "    columns=[f\"p{i}\" for i in range(3 * 32 * 32)],\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "target = pd.Series(labels, dtype=\"uint8\", name=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        m, _ = features.shape\n",
    "        k = np.random.randint(m)\n",
    "        img = (features.loc[k, :]\n",
    "                       .to_numpy()\n",
    "                       .reshape((3, 32, 32))\n",
    "                       .transpose(1, 2, 0))\n",
    "        _ = axes[i, j].imshow(img)\n",
    "        _ = axes[i, j].set_title(CLASS_LABELS[target[k]])\n",
    "\n",
    "fig.suptitle(\"Random CIFAR-10 images\", x=0.5, y=1.0, fontsize=25)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Test Dataset\n",
    "\n",
    "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s).\n",
    "\n",
    "## If you might refresh data in the future...\n",
    "\n",
    "...then you want to use some particular hashing function to compute the hash of a unique identifier for each observation of data and include the observation in the test set if resulting hash value is less than some fixed percentage of the maximum possible hash value for your algorithm. This way even if you fetch more data, your test set will never include data that was previously included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "\n",
    "def in_testing_data(identifier, test_size):\n",
    "    _hash = zlib.crc32(bytes(identifier))\n",
    "    return _hash & 0xffffffff < test_size * 2**32\n",
    "\n",
    "\n",
    "def split_train_test_by_id(data, test_size, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda identifier: in_testing_data(identifier, test_size))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If this is all the data you will ever have...\n",
    "\n",
    "...then you can just set a seed for the random number generator and then randomly split the data. Scikit-Learn has a [`model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) module that contains tools for splitting datasets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SEED_GENERATOR = np.random.RandomState(SEED)\n",
    "\n",
    "\n",
    "def generate_seed():\n",
    "    return SEED_GENERATOR.randint(np.iinfo(\"uint16\").max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "_seed = generate_seed()\n",
    "_random_state = np.random.RandomState(_seed)\n",
    "train_features, test_features, train_target, test_target = model_selection.train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size=1e-1,\n",
    "    random_state=_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "Data for individual pixels is stored as integers between 0 and 255. Neural network models work best when numerical features are scaled. To rescale the raw features we can use tools from the [Scikit-Learn preprocessing module](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "_min_max_scaler_hyperparameters = {\n",
    "    \"feature_range\": (0, 1),\n",
    "}\n",
    "\n",
    "preprocessor = preprocessing.MinMaxScaler(**_min_max_scaler_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_features = (preprocessor.fit_transform(train_features)\n",
    "                                           .astype(\"float32\"))\n",
    "preprocessed_train_target = (train_target.to_numpy()\n",
    "                                         .astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical ML Benchmark Model\n",
    "\n",
    "We have several of these from yesterday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "_estimator_hyperpararmeters = {\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": True,\n",
    "    \"max_samples\": 0.9,\n",
    "    \"random_state\": np.random.RandomState(_seed),\n",
    "}\n",
    "estimator = ensemble.RandomForestClassifier(**_estimator_hyperpararmeters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = estimator.fit(preprocessed_train_features, preprocessed_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch\n",
    "\n",
    "## Split the training data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_seed = generate_seed()\n",
    "_random_state = np.random.RandomState(_seed)\n",
    "preprocessed_train_features, preprocessed_val_features, preprocessed_train_target, preprocessed_val_target = (\n",
    "    model_selection.train_test_split(preprocessed_train_features,\n",
    "                                     preprocessed_train_target,\n",
    "                                     test_size=1e-1,\n",
    "                                     random_state=_random_state)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_val_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next let's create a simple model using nothing but [PyTorch tensor operations](https://pytorch.org/docs/stable/tensors.html). PyTorch uses `torch.tensor` rather than `numpy.ndarray` so we need to convert data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_target = torch.from_numpy(preprocessed_train_target)\n",
    "preprocessed_train_features = torch.from_numpy(preprocessed_train_features)\n",
    "\n",
    "preprocessed_val_target = torch.from_numpy(preprocessed_val_target)\n",
    "preprocessed_val_features = torch.from_numpy(preprocessed_val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n",
    "\n",
    "For the weights, we set `requires_grad` after the initialization, since we don’t want that step included in the gradient. (Note that a trailling `_` in PyTorch signifies that the operation is performed _in-place_.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples, number_features = preprocessed_train_features.shape\n",
    "\n",
    "# using Xavier initialization (divide weights by sqrt(number_features))\n",
    "weights = torch.randn(number_features, 10) / number_features**0.5\n",
    "weights.requires_grad_() # trailing underscore indicates in-place operation\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) in a model! So we will start by writing a function to peform matrix multiplication and broadcasted addition called `linear_transformation`. We will also need an activation function, so we’ll write a function called `log_softmax_activation` and use it. \n",
    "\n",
    "**N.B.** Although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transformation(X):\n",
    "    return X @ weights + bias\n",
    "\n",
    "def log_softmax_activation(X):\n",
    "    return X - X.exp().sum(-1).log().unsqueeze(-1)\n",
    "    \n",
    "def logistic_regression(X):\n",
    "    Z = linear_transformation(X)\n",
    "    return log_softmax_activation(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the `@` stands for the dot product operation. We will call our function on one batch of data (in this case, 64 images). Note that our predictions won’t be any better than random at this stage, since we start with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output = logistic_regression(preprocessed_train_features[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the `output` tensor contains not only the tensor values, but also a gradient function, `grad_fn`. We’ll use this later to do back propagation to update the model parameters.\n",
    "\n",
    "Let’s implement `negative_log_likelihood` to use as the loss function. Again, we can just use standard Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(output, target):\n",
    "    m, _ = output.shape\n",
    "    return -output[range(m), target].mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood(output, preprocessed_train_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also implement a function to calculate the `accuracy` of our model: for each prediction, if the index with the largest value matches the target value, then the prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    return (predictions == target).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes we can compute the accuracy of our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(output, preprocessed_train_target[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a training loop. For each iteration, we will:\n",
    "\n",
    "* select a mini-batch of data (of size `batch_size`)\n",
    "* use the model to make predictions\n",
    "* calculate the loss\n",
    "* `loss.backward()` updates the gradients of the model.\n",
    "\n",
    "We now use these gradients to update the weights and bias (i.e., model parameters). We do this within the `torch.no_grad()` context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch’s Autograd records operations [here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. loss.backward() adds the gradients to whatever is already stored, rather than replacing them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_fn = logistic_regression\n",
    "loss_fn = negative_log_likelihood\n",
    "\n",
    "number_epochs = 15\n",
    "number_batches = (number_samples - 1) // batch_size + 1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "for epoch in range(number_epochs):\n",
    "    for batch in range(number_batches):\n",
    "        \n",
    "        # forward pass\n",
    "        start = batch * batch_size\n",
    "        X = preprocessed_train_features[start:(start + batch_size)]\n",
    "        y = preprocessed_train_target[start:(start + batch_size)]\n",
    "        loss = loss_fn(model_fn(X), y)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            weights -= learning_rate * weights.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch! Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.functional`\n",
    "\n",
    "We will now refactor our code using [torch.nn](https://pytorch.org/docs/stable/nn.html) modules to make it more concise and flexible. The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from [torch.nn.functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional).\n",
    "\n",
    "Since we are using negative log likelihood loss and log softmax activation in this tutorial, we can use [torch.nn.functional.cross_entropy](https://pytorch.org/docs/stable/nn.html#cross-entropy) which combines the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linear_transformation(preprocessed_train_features)\n",
    "F.cross_entropy(Z, preprocessed_train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Module`\n",
    "\n",
    "Next up, we’ll use [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#module) and [torch.nn.Parameter](https://pytorch.org/docs/stable/nn.html#parameters), for a clearer and more concise training loop. In this case, we want to create a class that holds our weights, bias, and method for the forward step. `torch.nn.Module` has a number of attributes and methods (such as `parameters()` and `zero_grad()`) which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._weights = nn.Parameter(torch.randn(3 * 32 * 32, 10) / (3 * 32 * 32)**0.5)\n",
    "        self._bias = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return X @ self._weights + self._bias\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re now using an object instead of just using a function, we first have to instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the loss in the same way as before. Note that `torch.nn.Module` objects are used as if they are functions (i.e they are callable), but behind the scenes Pytorch will call the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(model_fn(preprocessed_train_features), preprocessed_train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in our training loop we had to update the values for each parameter by name and manually zero out the grads for each parameter separately.  With our refactoring we can take advantage of `model_fn.parameters()` and `model_fn.zero_grad()` (which are both defined by PyTorch for `torch.nn.Module` base class!) to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model.\n",
    "\n",
    "In order to facilitate re-use and continued refactoring, we can encapsulate the logic of our deep learning pipeline in the following functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, learning_rate, X_batch, y_batch):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for parameter in model_fn.parameters():\n",
    "            parameter -= learning_rate * parameter.grad\n",
    "        model_fn.zero_grad()\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, X, y, learning_rate=1e-2, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, learning_rate, X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, preprocessed_train_features, preprocessed_train_target, number_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.nn.Linear`\n",
    "\n",
    "Instead of defining and initializing `self._weights` and `self._bias`, and calculating `X  @ self._weights + self._bias`, we will instead use the Pytorch class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#linear) to define a linear layer which does all that for us. Pytorch has many types of predefined layers that can greatly simplify our code, and since the library code is highly optimized using PyTorch's predefined layers often makes our code faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear_layer = nn.Linear(3 * 32 * 32, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._linear_layer(X)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, preprocessed_train_features, preprocessed_train_target, number_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring using `torch.optim`\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, [torch.optim](https://pytorch.org/docs/stable/optim.html). We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter. Also note that now the `learning_rate` is a parameter of the optimizer and we do not need to manually pass it as an argument to the `fit` and `partial_fit` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch, opt):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad() # don't forget to reset the gradient after each batch!\n",
    "\n",
    "        \n",
    "def fit(model_fn, loss_fn, X, y, opt, number_epochs=2, batch_size=64):\n",
    "    number_samples, _ = X.shape \n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch = X[start:(start + batch_size)]\n",
    "            y_batch = y[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, preprocessed_train_features, preprocessed_train_target, opt, number_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.utils.data.TensorDataSet`\n",
    "\n",
    "The [torch.utils.data](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) module contains a number of useful classes that we can use to further simplify our code. PyTorch has an abstract `Dataset` class. A Dataset can be anything that has a `__len__` function (called by Python’s standard `len` function) and a `__getitem__` function as a way of indexing into it.\n",
    "\n",
    "PyTorch’s `TensorDataset` is a `Dataset` wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_set, number_samples, opt, number_epochs=2, batch_size=64):\n",
    "    number_batches = (number_samples - 1) // batch_size + 1\n",
    "    for epoch in range(number_epochs):\n",
    "        for batch in range(number_batches):\n",
    "            start = batch * batch_size\n",
    "            X_batch, y_batch = data_set[start:(start + batch_size)]\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "train_dataset = data.TensorDataset(preprocessed_train_features, preprocessed_train_target)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the annoying dependence on number of samples!\n",
    "fit(model_fn, loss_fn, train_dataset, number_samples, opt, number_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.utils.data.DataLoader`\n",
    "\n",
    "Pytorch’s `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. `DataLoader` makes it easier to iterate over batches. Rather than having to use `data_set[start:(start + batch_size)]`, the `DataLoader` gives us each minibatch automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, data_loader, opt, number_epochs=2):\n",
    "    for epoch in range(number_epochs):\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we no longer have the annoying dependency on number of samples!\n",
    "fit(model_fn, loss_fn, train_data_loader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Pytorch’s `torch.nn.Module`, `torch.nn.Parameter`, `Dataset`, and `DataLoader`, our training loop is now dramatically smaller and easier to understand. Let’s now try to add the basic features necessary to create effecive models in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Validation\n",
    "\n",
    "In the first part of this tutorial, we were just trying to get a reasonable training loop set up for use on our training data. In reality, you always should also have a validation set, in order to identify if you are overfitting.\n",
    "\n",
    "Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
    "\n",
    "We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, model_fn, loss_fn, val_data_loader):\n",
    "    model_fn.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_losses, batch_sizes = zip(*[(loss_fn(model_fn(X), y), len(X)) for X, y in val_data_loader])\n",
    "        val_loss = np.sum(np.multiply(batch_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "        print(f\"Training epoch: {epoch}, Validation loss: {val_loss}\")\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, train_data_loader, opt, val_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in train_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if val_data_loader is not None:\n",
    "            validate(epoch, model_fn, loss_fn, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LogisticRegression()\n",
    "loss_fn = F.cross_entropy\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2)\n",
    "\n",
    "val_dataset = data.TensorDataset(preprocessed_val_features, preprocessed_val_target)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn, loss_fn, train_data_loader, opt, val_data_loader, number_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = loss_fn(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "training_accuracy = accuracy(model_fn(preprocessed_train_features), preprocessed_train_target)\n",
    "\n",
    "print(f\"Training loss: {training_loss}\")\n",
    "print(f\"Training accuracy: {training_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Logging Accuracy during Validation\n",
    "\n",
    "Make the necessary changes to the `validation` function so that you log out your model's accuracy on the validation data after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Logging Accuracy during Validation\n",
    "\n",
    "Train your model for 15-20 epochs. Do you think the model is overfitting or underfitting? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switching to CNN\n",
    "\n",
    "We are now going to build our neural network with three convolutional-subsampling layers. Because none of the functions in the previous section assume anything about the model form, we’ll be able to use them to train a CNN without any modification!\n",
    "\n",
    "The first architecture that we will implement is the classic [LeNet-5](https://www.datasciencecentral.com/lenet-5-a-classic-cnn-architecture/)  architecture. We will use Pytorch’s predefined [torch.nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d) class as our convolutional layer. We define a CNN with 3 convolutional layers. Each convolution is followed by a [hyperbolic tangent](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh) non-linear activation function and [average pooling](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d). After the three convolutional-subsampling layers, we add a couple of densely connected linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._conv1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0)\n",
    "        self._conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self._dense1 = nn.Linear(400, 120)\n",
    "        self._dense2 = nn.Linear(120, 84)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, 3, 32, 32) # implicit knowledge of CIFAR-10 data shape!\n",
    "        X = F.avg_pool2d(F.tanh(self._conv1(X)), 2)\n",
    "        X = F.avg_pool2d(F.tanh(self._conv2(X)), 2)\n",
    "        X = X.view(X.size(0), -1)\n",
    "        X = F.tanh(self._dense1(X))\n",
    "        X = self._dense2(X)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we can re-use the loss function as well as trainig and validation data loaders\n",
    "fit(model_fn, loss_fn, train_data_loader, opt, val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor using `torch.nn.Sequential`\n",
    "\n",
    "PyTorch has another handy class we can use to simply our code: [torch.nn.Sequential](https://pytorch.org/docs/stable/nn.html#sequential). A `Sequential` object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our neural network.\n",
    "\n",
    "To take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn’t have a view layer, and we need to create one for our network. `LambdaLayer` will create a layer that we can then use when defining a network with `Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    LambdaLayer(lambda X: X.view(-1, 3, 32, 32)),\n",
    "    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1)),\n",
    "    nn.Linear(400, 120),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    train_data_loader,\n",
    "    opt,\n",
    "    val_data_loader,\n",
    "    number_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize our pipeline by wrapping our DataLoader\n",
    "\n",
    "Our CNN is fairly concise, but it only works with CIFAR-10, because it assumes the input is a 3 * 32 * 32 long vector. Let’s get rid of this assumption, so our model works with any three channel image. First, we can remove the initial Lambda layer by moving the data preprocessing into a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    \n",
    "    def __init__(self, data_loader, f):\n",
    "        self._data_loader = data_loader\n",
    "        self._f = f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in iter(self._data_loader):\n",
    "            yield self._f(*batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1)),\n",
    "    nn.Linear(400, 120),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "_preprocess = lambda X, y: (X.view(-1, 3, 32, 32), y)\n",
    "train_data_loader = WrappedDataLoader(train_data_loader, _preprocess)\n",
    "val_data_loader = WrappedDataLoader(val_data_loader, _preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    train_data_loader,\n",
    "    opt,\n",
    "    val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a learning rate scheduler\n",
    "\n",
    "Adjusting the learning rate is often critical to achieving good convergence to a local optimum. Fortunately, adjusting the learning rate using PyTorch requires only minor modifications to our training loop. While the \"best\" way to adjust the learning rate is nearly always problem specific, starting with larger values and then decaying the learning rate each epoch is often a good strategy to try first. See the official PyTorch documentation for more on [tuning learning rates](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.lr_scheduler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model_fn, loss_fn, train_data_loader, opt, lr_scheduler, val_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in train_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        if val_data_loader is not None:\n",
    "            validate(epoch, model_fn, loss_fn, val_data_loader)\n",
    "            \n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.9, verbose=True)\n",
    "\n",
    "_preprocess = lambda X, y: (X.view(-1, 3, 32, 32), y)\n",
    "train_data_loader = WrappedDataLoader(train_data_loader, _preprocess)\n",
    "val_data_loader = WrappedDataLoader(val_data_loader, _preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    train_data_loader,\n",
    "    opt,\n",
    "    lr_scheduler,\n",
    "    val_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different architectures\n",
    "\n",
    "In practice, it is unlikely that you will be designing your own neural network architectures from scratch. Instead you will be starting from some pre-existing neural network architecture. The [torchvision](https://pytorch.org/vision/stable/) project contains a number of neural network architectures that have found widespread use in computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = models.resnet18(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model with just a few CPUs would be impossible. In the next section we will see how to train large models like this using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
