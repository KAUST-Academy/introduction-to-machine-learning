{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import pickle\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torchinfo\n",
    "import torchmetrics\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## CIFAR-10 Dataset\n",
    "\n",
    "The original [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"../data/\")\n",
    "RAW_DATA_DIR = DATA_DIR / \"cifar-10\"\n",
    "URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(RAW_DATA_DIR / \"cifar-10-python.tar.gz\", \"wb\") as f:\n",
    "    response = requests.get(URL)\n",
    "    f.write(response.content)\n",
    "\n",
    "with tarfile.open(RAW_DATA_DIR / \"cifar-10-python.tar.gz\", \"r:gz\") as f:\n",
    "    f.extractall(RAW_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We will load the data using the [Pandas](https://pandas.pydata.org/) library. Highly recommend the most recent edition of [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/) by Pandas creator Wes Mckinney for anyone interested in learning how to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = []\n",
    "_labels = []\n",
    "filepaths = glob.glob(\"../data/cifar-10/cifar-10-batches-py/*_batch*\")\n",
    "for filepath in sorted(filepaths):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        _batch = pickle.load(f, encoding=\"latin1\")\n",
    "        _data.append(_batch[\"data\"])\n",
    "        _labels.extend(_batch[\"labels\"])\n",
    "\n",
    "# each image has 3 channels with height and width of 32 pixels\n",
    "features = pd.DataFrame(\n",
    "    np.vstack(_data),\n",
    "    columns=[f\"p{i}\" for i in range(3 * 32 * 32)],\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "target = pd.Series(_labels, dtype=\"uint8\", name=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, sharex=True, sharey=True, figsize=(15, 15))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        m, _ = features.shape\n",
    "        k = np.random.randint(m)\n",
    "        img = (features.loc[k, :]\n",
    "                       .to_numpy()\n",
    "                       .reshape((3, 32, 32))\n",
    "                       .transpose(1, 2, 0))\n",
    "        _ = axes[i, j].imshow(img)\n",
    "        _ = axes[i, j].set_title(CLASS_LABELS[target[k]])\n",
    "\n",
    "fig.suptitle(\"Random CIFAR-10 images\", x=0.5, y=1.0, fontsize=25)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train, Val, and Test Data\n",
    "\n",
    "Before we look at the data any further, we need to create a test set, put it aside, and never look at it (until we are ready to test our trainined machine learning model!). Why? We don't want our machine learning model to memorize our dataset (this is called overfitting). Instead we want a model that will generalize well (i.e., make good predictions) for inputs that it didn't see during training. To do this we hold split our dataset into training and testing datasets. The training dataset will be used to train our machine learning model(s) and the testing dataset will be used to make a final evaluation of our machine learning model(s). We also need to create a validation dataset for tuning hyperparameters and deciding when to stop training.\n",
    "\n",
    "## If you might refresh data in the future...\n",
    "\n",
    "...then you want to use some particular hashing function to compute the hash of a unique identifier for each observation of data and include the observation in the test set if resulting hash value is less than some fixed percentage of the maximum possible hash value for your algorithm. This way even if you fetch more data, your test set will never include data that was previously included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "\n",
    "def in_holdout_data(identifier, test_size):\n",
    "    _hash = zlib.crc32(bytes(identifier))\n",
    "    return _hash & 0xffffffff < test_size * 2**32\n",
    "\n",
    "\n",
    "def split_data_by_id(data, test_size, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_holdout_set = ids.apply(lambda identifier: in_holdout_data(identifier, test_size))\n",
    "    return data.loc[~in_holdout_set], data.loc[in_holdout_set]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If this is all the data you will ever have...\n",
    "\n",
    "...then you can just set a seed for the random number generator and then randomly split the data. Scikit-Learn has a [`model_selection`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) module that contains tools for splitting datasets. First, split the dataset into training and testing datasets. Next split the training dataset into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "SEED_GENERATOR = np.random.RandomState(SEED)\n",
    "\n",
    "\n",
    "def generate_seed():\n",
    "    return SEED_GENERATOR.randint(np.iinfo(\"uint16\").max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "_seed = generate_seed()\n",
    "_random_state = np.random.RandomState(_seed)\n",
    "_train_features, test_features, _train_target, test_target = model_selection.train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    test_size=1e-1,\n",
    "    random_state=_random_state\n",
    ")\n",
    "\n",
    "train_features, val_features, train_target, val_target = model_selection.train_test_split(\n",
    "    _train_features,\n",
    "    _train_target,\n",
    "    test_size=1e-1,\n",
    "    random_state=_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "When working with GPUs we need to tell PyTorch which device to use when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the components of our training loop that we developed in this morning session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    return torchmetrics.functional.accuracy(output, target)\n",
    "\n",
    "\n",
    "def partial_fit(model_fn, loss_fn, X_batch, y_batch, opt):\n",
    "    # forward pass\n",
    "    loss = loss_fn(model_fn(X_batch), y_batch)\n",
    "\n",
    "    # back propagation\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad() # don't forget to reset the gradient after each batch!\n",
    "    \n",
    "\n",
    "def validate(model_fn, loss_fn, data_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_accs = []\n",
    "        batch_losses = []\n",
    "        \n",
    "        for X, y in data_loader:\n",
    "            batch_accs.append(accuracy(model_fn(X), y))\n",
    "            batch_losses.append(loss_fn(model_fn(X), y))\n",
    "        \n",
    "        avg_accuracy = (torch.stack(batch_accs)\n",
    "                             .mean())\n",
    "        avg_loss = (torch.stack(batch_losses)\n",
    "                         .mean())\n",
    "\n",
    "    return avg_accuracy, avg_loss\n",
    "\n",
    "\n",
    "def fit(model_fn, loss_fn, train_data_loader, opt, lr_scheduler, val_data_loader=None, number_epochs=2):\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        # train the model\n",
    "        model_fn.train()\n",
    "        for X_batch, y_batch in train_data_loader:\n",
    "            partial_fit(model_fn, loss_fn, X_batch, y_batch, opt)\n",
    "        \n",
    "        # compute validation loss after each training epoch\n",
    "        model_fn.eval()\n",
    "        if val_data_loader is not None:\n",
    "            val_acc, val_loss = validate(model_fn, loss_fn, val_data_loader)\n",
    "        print(f\"Training epoch: {epoch}, Validation accuracy: {val_acc}, Validation loss: {val_loss}\")\n",
    "\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we introduce a `CustomDataset` to better encapsulate data preprocessing transformations using PyTorch primitives instead of Scikit-Learn. We also reuse the `LambdaLayer` and the `WrappedDataLoader` classes from this morning session. However, instead of using the `WrappedDataLoader` to implement data preprocessing steps, we will instead use the class to send our training data batches from the CPU to the GPU during the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, features, target, transforms = None):\n",
    "        self._data = (features.to_numpy()\n",
    "                              .reshape(-1, 3, 32, 32)\n",
    "                              .transpose(0, 2, 3, 1))\n",
    "        self._target = target.to_numpy()\n",
    "        self._transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X, y = self._data[index], self._target[index]\n",
    "        return (self._transforms(X), y) if self._transforms is not None else (X, y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self._f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self._f(X)\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    \n",
    "    def __init__(self, data_loader, f):\n",
    "        self._data_loader = data_loader\n",
    "        self._f = f\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in iter(self._data_loader):\n",
    "            yield self._f(*batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LeNet-5 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "    nn.Tanh(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    LambdaLayer(lambda X: X.view(X.size(0), -1)),\n",
    "    nn.Linear(400, 120),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "_ = model_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same loss function from last time\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# define some preprocessing transforms (done on CPU!)\n",
    "_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# move the tensor from the CPU to the GPU\n",
    "_to_device = lambda X, y: (X.to(device), y.to(device))\n",
    "\n",
    "# define the datasets and dataloaders\n",
    "_train_dataset = CustomDataset(train_features, train_target, _transforms)\n",
    "_train_data_loader = data.DataLoader(_train_dataset, batch_size=64, shuffle=True)\n",
    "train_data_loader = WrappedDataLoader(_train_data_loader, _to_device)\n",
    "\n",
    "_val_dataset = CustomDataset(val_features, val_target, _transforms)\n",
    "_val_data_loader = data.DataLoader(_val_dataset, batch_size=128, shuffle=False)\n",
    "val_data_loader = WrappedDataLoader(_val_data_loader, _to_device)\n",
    "\n",
    "_test_dataset = CustomDataset(test_features, test_target, _transforms)\n",
    "_test_data_loader = data.DataLoader(_test_dataset, batch_size=128, shuffle=False)\n",
    "test_data_loader = WrappedDataLoader(_test_data_loader, _to_device)\n",
    "\n",
    "# define the optimizer and the learning rate scheduler\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-2, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=0.9, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    train_data_loader,\n",
    "    opt,\n",
    "    lr_scheduler,\n",
    "    val_data_loader,\n",
    "    number_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy, average_loss = validate(model_fn, loss_fn, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build your own neural network\n",
    "\n",
    "Modify the LeNet-5 archtiecture as you see fit in order to gain experience building your own neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with different batch sizes\n",
    "\n",
    "Train your model for 10 epochs with different batch sizes: 1, 4, 16, 64, 256. Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Experiment with different learning rate schedulers\n",
    "\n",
    "Train your model for 10 epochs with different batch size of 64 but experiment with different learning rate schedulers. Does one learning rate scheduler outperform the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different architectures\n",
    "\n",
    "In practice, it is unlikely that you will be designing your own neural network architectures from scratch. Instead you will be starting from some pre-existing neural network architecture. The [torchvision](https://pytorch.org/vision/stable/) project contains a number of neural network architectures that have found widespread use in computer vision applications.\n",
    "\n",
    "For the remainder of this notebook we will be using the [ResNet-18](https://arxiv.org/pdf/1512.03385.pdf) model which was developed in 2015. The ResNet family of models were designed to be trained on larger images (224 x 224) and a larger number of classes (1000) so we need to make some small modifications in order to adapt this network for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = models.resnet18(num_classes=10)\n",
    "model_fn.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1,1), padding=(1,1), bias=False)\n",
    "_ = model_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model_fn, input_size=(64, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same loss function from last time\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# define some preprocessing transforms (done on CPU!)\n",
    "_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# move the tensor from the CPU to the GPU\n",
    "_to_device = lambda X, y: (X.to(device), y.to(device))\n",
    "\n",
    "# define the datasets and dataloaders\n",
    "_train_dataset = CustomDataset(train_features, train_target, _transforms)\n",
    "_train_data_loader = data.DataLoader(_train_dataset, batch_size=128, shuffle=True)\n",
    "train_data_loader = WrappedDataLoader(_train_data_loader, _to_device)\n",
    "\n",
    "_val_dataset = CustomDataset(val_features, val_target, _transforms)\n",
    "_val_data_loader = data.DataLoader(_val_dataset, batch_size=256, shuffle=False)\n",
    "val_data_loader = WrappedDataLoader(_val_data_loader, _to_device)\n",
    "\n",
    "_test_dataset = CustomDataset(test_features, test_target, _transforms)\n",
    "_test_data_loader = data.DataLoader(_test_dataset, batch_size=256, shuffle=False)\n",
    "test_data_loader = WrappedDataLoader(_test_data_loader, _to_device)\n",
    "\n",
    "# define the optimizer and the learning rate scheduler\n",
    "opt = optim.SGD(model_fn.parameters(), lr=1e-1, momentum=0.9)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(opt, step_size=2, gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    train_data_loader,\n",
    "    opt,\n",
    "    lr_scheduler,\n",
    "    val_data_loader,\n",
    "    number_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy, average_loss = validate(model_fn, loss_fn, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_accuracy, average_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
