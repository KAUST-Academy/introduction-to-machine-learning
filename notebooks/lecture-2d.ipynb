{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSPJY3m3gkUS"
      },
      "source": [
        "# Select and Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import requests\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "def download_data(url, data_dir):\n",
        "    with open(data_dir / \"housing.tgz\", 'wb') as f:\n",
        "        response = requests.get(url)\n",
        "        f.write(response.content)\n",
        "\n",
        "\n",
        "def extract_data(data_dir):\n",
        "    with tarfile.open(data_dir / \"housing.tgz\") as tgz:\n",
        "        tgz.extractall(path=data_dir)\n",
        "\n",
        "\n",
        "# load the data\n",
        "url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "data_dir = pathlib.Path(\"./sample_data\")\n",
        "data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "download_data(url, data_dir)\n",
        "extract_data(data_dir)\n",
        "housing_df = pd.read_csv(data_dir / \"housing\" / \"housing.csv\")\n",
        "\n",
        "# stratified sampling to match the income distribution\n",
        "housing_df[\"income_cat\"] = pd.cut(\n",
        "    housing_df[\"median_income\"],\n",
        "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "    labels=[0, 1, 2, 3, 4]\n",
        ")\n",
        "\n",
        "train_df, test_df = model_selection.train_test_split(\n",
        "    housing_df,\n",
        "    test_size=0.2,\n",
        "    stratify=housing_df.loc[:, \"income_cat\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_df.drop(\"income_cat\", axis=1, inplace=True)\n",
        "test_df.drop(\"income_cat\", axis=1, inplace=True)\n",
        "\n",
        "# split off the features and the target\n",
        "train_features_df = train_df.drop(\"median_house_value\", axis=1)\n",
        "train_targets = train_df.loc[:, \"median_house_value\"]"
      ],
      "metadata": {
        "id": "Kw4IQa5e97dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import base, cluster, compose, impute, metrics, pipeline, preprocessing\n",
        "\n",
        "\n",
        "class ClusterSimilarity(base.BaseEstimator, base.TransformerMixin):\n",
        "\n",
        "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.gamma = gamma\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        kmeans = cluster.KMeans(\n",
        "            self.n_clusters,\n",
        "            n_init=10,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.kmeans_ = kmeans.fit(X, sample_weight=sample_weight)\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        similarities = (\n",
        "            metrics.pairwise\n",
        "                   .rbf_kernel(\n",
        "                       X,\n",
        "                       Y=self.kmeans_.cluster_centers_,\n",
        "                       gamma=self.gamma\n",
        "                   )\n",
        "        )\n",
        "        return similarities\n",
        "\n",
        "    def get_feature_names_out(self, names=None):\n",
        "        return [f\"cluster_{i:02d}_similarity\" for i in range(self.n_clusters)]\n",
        "\n",
        "\n",
        "def column_ratio(df):\n",
        "    return df.iloc[:, 0] / df.iloc[:, 1]\n",
        "\n",
        "\n",
        "def ratio_name(function_transformer, feature_names_in):\n",
        "    return [\"ratio\"]  # feature names out\n",
        "\n",
        "\n",
        "def make_ratio_pipeline():\n",
        "    ratio_pipeline = (\n",
        "        pipeline.make_pipeline(\n",
        "            impute.SimpleImputer(strategy=\"median\"),\n",
        "            preprocessing.FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
        "            preprocessing.StandardScaler(),\n",
        "            verbose=True\n",
        "        ).set_output(\n",
        "            transform=\"pandas\"\n",
        "        )\n",
        "    )\n",
        "    return ratio_pipeline\n",
        "\n",
        "\n",
        "log_transform_pipeline = (\n",
        "    pipeline.make_pipeline(\n",
        "        impute.SimpleImputer(strategy=\"median\"),\n",
        "        preprocessing.FunctionTransformer(np.log, np.exp, feature_names_out=\"one-to-one\"),\n",
        "        preprocessing.StandardScaler()\n",
        "    ).set_output(\n",
        "        transform=\"pandas\"\n",
        "    )\n",
        ")\n",
        "\n",
        "cluster_similarity = (\n",
        "    ClusterSimilarity(\n",
        "        n_clusters=10,\n",
        "        gamma=1.,\n",
        "        random_state=42\n",
        "    ).set_output(\n",
        "        transform=\"pandas\"\n",
        "    )\n",
        ")\n",
        "\n",
        "categorical_pipeline = (\n",
        "    pipeline.make_pipeline(\n",
        "        impute.SimpleImputer(strategy=\"most_frequent\"),\n",
        "        preprocessing.OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "    ).set_output(\n",
        "        transform=\"pandas\"\n",
        "    )\n",
        ")\n",
        "\n",
        "default_numeric_pipeline = (\n",
        "    pipeline.make_pipeline(\n",
        "        impute.SimpleImputer(strategy=\"median\"),\n",
        "        preprocessing.StandardScaler(),\n",
        "        verbose=True\n",
        "    ).set_output(\n",
        "        transform=\"pandas\"\n",
        "    )\n",
        ")\n",
        "\n",
        "preprocessing_pipeline = (\n",
        "    compose.ColumnTransformer(\n",
        "        [\n",
        "            (\"bedrooms\", make_ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
        "            (\"rooms_per_house\", make_ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
        "            (\"people_per_house\", make_ratio_pipeline(), [\"population\", \"households\"]),\n",
        "            (\"log\", log_transform_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n",
        "            (\"geo\", cluster_similarity, [\"latitude\", \"longitude\"]),\n",
        "            (\"categorical\", categorical_pipeline, compose.make_column_selector(dtype_include=object)),\n",
        "        ],\n",
        "        n_jobs=-1,\n",
        "        remainder=default_numeric_pipeline,\n",
        "        verbose=True\n",
        "    ).set_output(\n",
        "        transform=\"pandas\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "n7sye7gd975W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline"
      ],
      "metadata": {
        "id": "U40si4pekV-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPTfzJA7gkUS"
      },
      "source": [
        "## Training and Evaluating on the Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rjMsOKhgkUS"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "\n",
        "linear_regression_pipeline = pipeline.make_pipeline(\n",
        "    preprocessing_pipeline,\n",
        "    linear_model.LinearRegression()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regression_pipeline"
      ],
      "metadata": {
        "id": "iSX8jEsikbxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = linear_regression_pipeline.fit(train_features_df, train_targets)"
      ],
      "metadata": {
        "id": "UiwyqaVGkfOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT6-XmlAgkUS"
      },
      "source": [
        "Let's try the full preprocessing pipeline on a few training instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58YaibedgkUS"
      },
      "outputs": [],
      "source": [
        "linear_regression_pipeline.predict(train_features_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq7m60oggkUS"
      },
      "source": [
        "Compare against the actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tw9vTXWgkUT"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "train_predictions = linear_regression_pipeline.predict(train_features_df)\n",
        "\n",
        "linear_regression_rmse = (\n",
        "    metrics.mean_squared_error(\n",
        "        train_targets,\n",
        "        train_predictions,\n",
        "        squared=False\n",
        "    )\n",
        ")\n",
        "\n",
        "linear_regression_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_LaWYn8gkUT"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "\n",
        "decision_tree_pipeline = (\n",
        "    pipeline.make_pipeline(\n",
        "        preprocessing_pipeline,\n",
        "        tree.DecisionTreeRegressor(random_state=42)\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_pipeline"
      ],
      "metadata": {
        "id": "Up3vGcwbl4pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = decision_tree_pipeline.fit(train_features_df, train_targets)"
      ],
      "metadata": {
        "id": "m_layhsTl66I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gHS2BdYgkUT"
      },
      "outputs": [],
      "source": [
        "train_predictions = decision_tree_pipeline.predict(train_features_df)\n",
        "\n",
        "decision_tree_rmse = (\n",
        "    metrics.mean_squared_error(\n",
        "        train_targets,\n",
        "        train_predictions,\n",
        "        squared=False\n",
        "    )\n",
        ")\n",
        "\n",
        "decision_tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gII_h67ygkUT"
      },
      "source": [
        "## Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJRYfiWRgkUT"
      },
      "outputs": [],
      "source": [
        "decision_tree_scores = (\n",
        "    model_selection.cross_val_score(\n",
        "        decision_tree_pipeline,\n",
        "        train_features_df,\n",
        "        train_targets,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUuDMD-3gkUT"
      },
      "outputs": [],
      "source": [
        "decision_tree_rmse = pd.Series(-decision_tree_scores, name=\"rmse\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree_rmse.describe()"
      ],
      "metadata": {
        "id": "ed-xbIrXm7OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7BSt3FRgkUT"
      },
      "outputs": [],
      "source": [
        "linear_regression_scores = (\n",
        "    model_selection.cross_val_score(\n",
        "        linear_regression_pipeline,\n",
        "        train_features_df,\n",
        "        train_targets,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_regression_rmse = pd.Series(-linear_regression_scores, name=\"rmse\")\n",
        "linear_regression_rmse.describe()"
      ],
      "metadata": {
        "id": "MIrvDfrfnRT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVrrWzsfgkUT"
      },
      "outputs": [],
      "source": [
        "from sklearn import ensemble\n",
        "\n",
        "random_forest_pipeline = (\n",
        "    pipeline.make_pipeline(\n",
        "        preprocessing_pipeline,\n",
        "        ensemble.RandomForestRegressor(random_state=42),\n",
        "        verbose=True\n",
        "    )\n",
        ")\n",
        "\n",
        "random_forest_scores = (\n",
        "    model_selection.cross_val_score(\n",
        "        random_forest_pipeline,\n",
        "        train_features_df,\n",
        "        train_targets,\n",
        "        scoring=\"neg_root_mean_squared_error\",\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN4SwUtggkUT"
      },
      "outputs": [],
      "source": [
        "random_forest_rmse = pd.Series(-random_forest_scores, name=\"rmse\")\n",
        "random_forest_rmse.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVrA8_cfgkUT"
      },
      "source": [
        "Let's compare this RMSE measured using cross-validation (the \"validation error\") with the RMSE measured on the training set (the \"training error\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5ab91D1gkUT"
      },
      "outputs": [],
      "source": [
        "_ = random_forest_pipeline.fit(train_features_df, train_targets)\n",
        "\n",
        "predictions = random_forest_pipeline.predict(train_features_df)\n",
        "random_forest_rmse = (\n",
        "    metrics.mean_squared_error(\n",
        "        train_targets,\n",
        "        predictions,\n",
        "        squared=False\n",
        "    )\n",
        ")\n",
        "\n",
        "random_forest_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA_pgJjRgkUT"
      },
      "source": [
        "The training error is much lower than the validation error, which usually means that the model has overfit the training set. Another possible explanation may be that there's a mismatch between the training data and the validation data, but it's not the case here, since both came from the same dataset that we shuffled and split in two parts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}